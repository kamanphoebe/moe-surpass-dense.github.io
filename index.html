<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MoEs Surpass Dense LLMs</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="margin-bottom: 20px;">Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resource?</h1>
            <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Houyi Li<sup>1,2</sup></span>
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Ka Man Lo<sup>1</sup></span>
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Ziqi Wang<sup>1</sup>,</span>
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Zili Wang<sup>1</sup></span>
                <span class="author-block" style="color: rgb(26, 82, 130);">
                Wenzhen Zheng<sup>1</sup></span>
                <br>
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Shuigeng Zhou<sup>2</sup></span>
                <span class="author-block" style="color: rgb(26, 82, 130); padding-right: 20px;">
                Xiangyu Zhang<sup>1,3</sup></span>
                <span class="author-block" style="color: rgb(26, 82, 130);">
                Daxin Jiang<sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span><br></span>
                    <span class="author-block" style="font-size: 0.75rem; padding-right: 20px;"><sup>1</sup>StepFun</span>
                    <span class="author-block" style="font-size: 0.75rem; padding-right: 20px;"><sup>2</sup>Fudan University</span>
                    <span class="author-block" style="font-size: 0.75rem; padding-right: 20px;"><sup>3</sup>Megvii Technology</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="24" height="24" />
                </span>
                <span>Hugging Face</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. 
            However, can MoEs <em>surpass</em> dense architectures under strictly equal resource constraints â€” that is, when the total parameter count, training compute, and data budget are identical?
            This question remains under-explored despite its significant practical value and potential.
            In this paper, we propose a novel perspective and methodological framework to study this question thoroughly.
            First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance.
            Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource.
            More importantly, this optimal region remains consistent across different model sizes.
            Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data.
            We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <!-- Paper video. -->
            <h2 class="title is-3">Three-step Experimental Methodology</h2>
            <p>
              Drawing insights from a unified parameterization framework for model architecture (see our [paper]() for details), we propose a three-step experimental methodology:
              <ol>
                <li><strong>Search for an optimized architecture design</strong> to ensure each model candidate achieves its (near-)optimal performance.</li>
                <li> <strong>Explore the optimal activation rate (AR)</strong> based on the optimized model architecture, keeping the total parameters and compute budget fixed.</li>
                <li> <strong>Present a data reuse strategy</strong> to address the additional data demand of MoE models, thereby equating data resources.</li>
              </ol>
              We also analyze the efficacy of this framework on downstream tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <h2 class="title is-3">Optimal Activation Rate</h2>
            <p>
              Based on our optimized model backbones (see our [paper]() for details), we build a series of MoE models with non-vocabulary parameters $N \approx 2\text{B}$ (Figure 1) and $N \approx 7\text{B}$ (Figure 2). 
              Our experimental results identify the existence of an optimal AR region, where MoE models can outperform their dense counterparts under the same training budget.
              Furthermore, the optimal AR point within this region , $r_\text{a}^{**} \approx 20\%$, remains consistent across different model sizes. 
            </p>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- Your image here -->
                  <img src="static/images/2B_keep_data.png" alt="2B models with fixed data or activation rate" style="width: 55%; height: auto;"/>
                  <img src="static/images/2B_keep_compute.png" alt="2B models with fixed compute" style="width: 45%; height: auto;"/>
                  <figcaption>
                    Figure 1: Performance of $N \approx 2\text{B}$ models trained with varying data sizes $D$ and activation rates $r_\text{a}$.
                    <strong>(a)</strong> With a fixed $D$, the performance gain exhibits a <strong>non-linear</strong> dependence on the training budget $C$. 
                    Conversely, with a fixed $r_\text{a}$, increasing $D$ results in a <strong>linear</strong> performance gain. 
                    These findings identify an optimal activation rate, $r_\text{a}^{**} = 20\%$, that remains consistent across different values of $D$ when $N$ is constant.
                    <strong>(b)</strong> From the perspective of a fixed training compute $C$, the optimal activation rate $r_\text{a}^{**} = 20\%$ can be clearly observed.
                  </figcaption>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <h2 class="title is-3">Data Reuse Strategy</h2>
                  <p>
                    To eliminate the additional data demand for MoEs to outperform their dense counterparts, we investigate data reusability by training models for multiple epochs using a fixed, smaller dataset.
                    We explore two distinct data reuse schemes: the strict scheme ensures that both MoE and dense models are trained under <em>completely</em> equal conditions; 
                    the loose scheme relaxes the constraint of identical data volume by fixing the number of training epochs to 2.
                  </p>
                  <p>
                    As shown in Figure 2b, the strict data reuse scheme (blue dashed line) only marginally diminishes performance, and MoE models continue to outperform dense baselines.
                    Surprisingly, the loose scheme (green dashed line) often outperforms training on the unique dataset for a single epoch.
                  </p>
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item">
                      <!-- Your image here -->
                      <img src="static/images/7B_keep_data.png" alt="7B models with fixed data" style="width: 55%; max-width: 600px; height: auto;"/>
                      <img src="static/images/7B_keep_compute.png" alt="7B models with fixed compute" style="width: 45%; max-width: 600px; height: auto;"/>
                      <figcaption>
                        Figure 2: Performance of $N \approx 7\text{B}$ models trained with varying data sizes $D$ and activation rate $r_\text{a}$.
                        The optimal activation rate, $r_\text{a}^{**}=20\%$, align with the findings for the 2B models (Figure 1).
                        Additionally, compared to training on the unique dataset, the strict data reuse scheme shows only a slight performance reduction, while the loose scheme often yields better performance.
                      </figcaption>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <h2 class="title is-3">Analysis of Downstream Performance</h2>
            <p>
              To assess whether the optimal ARs generalize to downstream tasks, we conduct SFT on our 7B pre-trained models (trained w/ and w/o strict data reuse) 
              and evaluate both the pre-trained models and SFT-ed models on a total number of 29 benchmarks (Figure 3; Table 2).
              The results demonstrate the universality of the optimal AR point ($r_\text{a}^{**} = 20\%$) across different training phases and data domains.
              See our paper for more analyses.
            </p>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/7B_downstream_pretrain.png" alt="Downstream performance of 7B pretrained models" style="width: 100%; height: auto;"/>
                <br>
                <img src="static/images/7B_downstream_sft.png" alt="Downstream performance of 7B SFT-ed models" style="width: 100%; height: auto;"/>
                <figcaption>
                  Figure 3: Downstream performance of 7B models: pre-trained (<strong>top</strong>) and SFT-ed (<strong>middle and bottom</strong>) versions. 
                  Across all benchmark types, MoE models with $r_\text{a}=20\%$ outperform dense model trained with twice the compute, aligning with upstream observations that the optimal AR is 20%.
                </figcaption>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="section" style="background-color: #ffe6e6; width: 100%; padding: 20px;">
  <div class="container is-max-desktop"> 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p style="text-align: center; font-size: 1.0rem; font-family: 'Noto Sans', sans-serif; color: #838383;">
          Mixture-of-Experts can surpass dense LLMs under equal total parameters, compute, and data constraints, <br>
          provided that the backbones are optimized and the activation rates reside in the optimal region.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
